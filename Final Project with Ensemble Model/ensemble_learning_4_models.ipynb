{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DL Model\n",
    "import os, fnmatch\n",
    "import sys\n",
    "import librosa\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn import metrics\n",
    "\n",
    "#ML Model\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,roc_auc_score, accuracy_score, recall_score, f1_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# import cleaning\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path, offset):\n",
    "\ty, sr = librosa.load(audio_path, offset=offset, duration=3)\n",
    "\tS = librosa.feature.melspectrogram(\n",
    "\ty, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n",
    "\tmfccs = librosa.feature.mfcc(S=librosa.power_to_db(S), n_mfcc=40)\n",
    "\t# mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "\treturn mfccs\n",
    "\n",
    "def most_freq_dl(list):\n",
    "    # print(\"Ok1\")\n",
    "    return np.bincount(list).argmax()\n",
    "            \n",
    "MAX_SOUND_CLIP_DURATION=12 #sec  \n",
    "def audio_norm(data):\n",
    "    max_data = np.max(data)\n",
    "    min_data = np.min(data)\n",
    "    data = (data-min_data)/(max_data-min_data+0.0001)\n",
    "    return data-0.5\n",
    "\n",
    "# get audio data without padding highest qualify audio\n",
    "def load_file_data_without_change(folder,file_names, duration=3, sr=16000):\n",
    "    input_length=sr*duration\n",
    "    # function to load files and extract features\n",
    "    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n",
    "    data = []\n",
    "    for file_name in file_names:\n",
    "        try:\n",
    "            sound_file=folder+file_name\n",
    "            print (\"load file \",sound_file)\n",
    "            # use kaiser_fast technique for faster extraction\n",
    "            X, sr = librosa.load( sound_file,res_type='kaiser_fast') \n",
    "            dur = librosa.get_duration(y=X, sr=sr)\n",
    "            # extract normalized mfcc feature from data\n",
    "            mfccs_ml = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0) \n",
    "        except Exception as e:\n",
    "            print(\"Error encountered while parsing file: \", file_name)\n",
    "        feature = np.array(mfccs_ml).reshape([-1,1])\n",
    "        data.append(feature)\n",
    "    return data\n",
    "\n",
    "\n",
    "# get audio data with a fix padding may also chop off some file\n",
    "def load_file_data (folder,file_names, duration=12, sr=16000):\n",
    "    input_length=sr*duration\n",
    "    # function to load files and extract features\n",
    "    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n",
    "    data = []\n",
    "    for file_name in file_names:\n",
    "        try:\n",
    "            sound_file=folder+file_name\n",
    "            print (\"load file \",sound_file)\n",
    "            # use kaiser_fast technique for faster extraction\n",
    "            X, sr = librosa.load( sound_file, sr=sr, duration=duration,res_type='kaiser_fast') \n",
    "            dur = librosa.get_duration(y=X, sr=sr)\n",
    "            # pad audio file same duration\n",
    "            if (round(dur) < duration):\n",
    "                print (\"fixing audio lenght :\", file_name)\n",
    "                y = librosa.util.fix_length(X, input_length)                \n",
    "            #normalized raw audio \n",
    "            # y = audio_norm(y)            \n",
    "            # extract normalized mfcc feature from data\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0)             \n",
    "        except Exception as e:\n",
    "            print(\"Error encountered while parsing file: \", file_name)        \n",
    "        feature = np.array(mfccs).reshape([-1,1])\n",
    "        data.append(feature)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file to test\n",
    "ml_upload_folder_path = 'C:/Users/Manisha/BE_Project/Physionet_Dataset/training/training-a/'\n",
    "classify_file = 'C:/Users/Manisha/BE_Project/Physionet_Dataset/training/training-a/a0011.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x_test_dl = []\n",
    "x_test_dl.append(extract_features(classify_file, 0.5))\n",
    "x_test_dl = np.asarray(x_test_dl)\n",
    "# print(x_test_dl.shape[0])\n",
    "# print(x_test_dl.shape[1])\n",
    "# print(x_test_dl.shape[2])\n",
    "# print(x_test_dl.shape[3])\n",
    "x_test_dl = x_test_dl.reshape(x_test_dl.shape[0], x_test_dl.shape[1], x_test_dl.shape[2], 1)\n",
    "model = keras.models.load_model('heartbeat_classifier_categorical_crossentropy.h5')\n",
    "pred_class = model.predict_classes(x_test_dl, verbose=1)\n",
    "CNNprob = model.predict_proba(x_test_dl, verbose=1)\n",
    "# CNNprob = CNNprob[:,1:]\n",
    "# print(CNNprob)\n",
    "CNNpred = np.argmax(CNNprob, axis=1)\n",
    "CNNpred = CNNpred + 1\n",
    "# print(CNNpred)\n",
    "print(type(pred_class))\n",
    "scores =[]\n",
    "scores = [accuracy_score(CNNpred,pred_class)]\n",
    "\n",
    "# print(accuracy_score(CNNpred,[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load file  C:/Users/Manisha/BE_Project/Physionet_Dataset/training/training-a/a0011.wav\n"
     ]
    }
   ],
   "source": [
    "test_sounds = load_file_data(folder=ml_upload_folder_path,file_names=['a0011.wav'], duration=MAX_SOUND_CLIP_DURATION)\n",
    "test_labels = [-1 for items in test_sounds]\n",
    "testing_data_rf = np.squeeze(test_sounds)\n",
    "# print(testing_data_rf)\n",
    "testing_data_rf = testing_data_rf.reshape(1, -1)\n",
    "# loaded_model = pickle.load(open('ml_classifier_random_forest.pkl', 'rb'))\n",
    "# prediction_rf = loaded_model.predict(testing_data_rf)\n",
    "# KNNProb = loaded_model.predict_proba(testing_data_rf)\n",
    "# print(type(prediction_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_el(x_test_dl, testing_data_rf, dl_prediction_labels):\n",
    "    \n",
    "# #     KNNmodel = MLmodel\n",
    "# #     MLPmodel = DL1model\n",
    "# #     CNNmodel = DL2model\n",
    "    \n",
    "#     #Load saved models and PCA transformations\n",
    "#     MLmodel = pickle.load(open('ml_classifier_random_forest.pkl', 'rb'))\n",
    "#     DL1model = keras.models.load_model('heartbeat_classifier_binary_crossentropy.h5')\n",
    "#     DL2model = keras.models.load_model('heartbeat_classifier_categorical_crossentropy.h5')\n",
    "    \n",
    "#     #Make predictions with test data\n",
    "# #     scores = []\n",
    "   \n",
    "#     DL1pred = DL1model.predict_classes(x_test_dl)\n",
    "#     DL1prob = DL1model.predict_proba(x_test_dl)\n",
    "# #     scores += [accuracy_score(DL1pred,dl_prediction_labels)]\n",
    "#     DL2prob = DL2model.predict(x_test_dl)\n",
    "#     DL2prob = DL2prob[:,1:]\n",
    "#     DL2pred = np.argmax(DL2prob, axis=1)\n",
    "#     DL2pred = DL2pred + 1\n",
    "# #     scores += [accuracy_score(DL2pred,dl_prediction_labels)]\n",
    "    \n",
    "#     MLpred = MLmodel.predict(testing_data_rf)\n",
    "#     MLprob = MLmodel.predict_proba(testing_data_rf)\n",
    "# #     scores += [accuracy_score(MLpred,dl_prediction_labels)]\n",
    "    \n",
    "#     #Ensemble learning/voting system\n",
    "#     final_pred = []\n",
    "    \n",
    "#     for i in range(len(MLpred)):\n",
    "#         print(\"DL2pred: \", DL2pred[i], \"DL1pred: \", DL1pred[i], \"MLpred: \", MLpred[i])\n",
    "#         if MLpred[i]==2:\n",
    "#             MLpred[i] = 0\n",
    "#         if DL2pred[i]==MLpred[i] and MLpred[i]==DL1pred[i]:\n",
    "#             final_pred += [DL2pred[i]]\n",
    "#         elif DL2pred[i]==MLpred[i] or MLpred[i]==DL1pred[i] or DL2pred[i]==DL1pred[i]:\n",
    "#             if DL2pred[i]==MLpred[i] or DL2pred[i]==DL1pred[i]:\n",
    "#                 final_pred += [DL2pred[i]]\n",
    "#             elif MLpred[i]==DL2pred[i] or MLpred[i]==DL1pred[i]:\n",
    "#                 final_pred += [MLpred[i]]\n",
    "#             elif DL1pred[i]==DL2pred[i] or DL1pred[i]==MLpred[i]:\n",
    "#                 final_pred += [DL1pred[i]]\n",
    "#             else:\n",
    "#                 print('bug')\n",
    "#         else:\n",
    "#             print(\"DL2prob[i]: \", DL2prob[i], \"DL1prob[i]:\", DL1prob[i], \"MLprob[i]: \", MLprob[i])\n",
    "#             DL1prob_max = DL1prob.max()\n",
    "#             MLprob_max = MLprob.max()\n",
    "#             prob = DL2prob[i] + DL1prob_max + MLprob_max\n",
    "#             pred = np.argmax(prob)\n",
    "#             pred = pred+1\n",
    "#             final_pred += [pred]\n",
    "    \n",
    "#     #Outputs\n",
    "#     final_pred = np.array(final_pred)\n",
    "# #     score = accuracy_score(final_pred,dl_prediction_labels)\n",
    "# #     cm = confusion_matrix(final_pred,dl_prediction_labels)\n",
    "#     return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "  \n",
    "def most_frequent(List):\n",
    "    occurence_count = Counter(List)\n",
    "#     print(\"occurence_count: \",occurence_count)\n",
    "    return occurence_count.most_common(1)[0][0], occurence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occurence_count:  Counter({0: 4, 1: 4})\n",
      "True\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "my_list = [0,0,0,0,1, 1, 1, 1]\n",
    "most_freq_item, occ_cnt = most_frequent(my_list)\n",
    "print(occ_cnt[0] == occ_cnt[1])\n",
    "print(occ_cnt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_el(x_test_dl, testing_data_rf, dl_prediction_labels):\n",
    "    \n",
    "#     KNNmodel = ML1model\n",
    "#     MLPmodel = DL1model\n",
    "#     CNNmodel = DL2model\n",
    "    \n",
    "    #Load saved models and PCA transformations\n",
    "    ML1model = pickle.load(open('ml_classifier_random_forest.pkl', 'rb'))\n",
    "    ML2model = pickle.load(open('ml_classifier_LogisticRegression.pkl', 'rb'))\n",
    "    DL1model = keras.models.load_model('heartbeat_classifier_binary_crossentropy.h5')\n",
    "    DL2model = keras.models.load_model('heartbeat_classifier_categorical_crossentropy.h5')\n",
    "    \n",
    "    #Make predictions with test data\n",
    "    scores = []\n",
    "   \n",
    "    DL1pred = DL1model.predict_classes(x_test_dl)\n",
    "    DL1prob = DL1model.predict_proba(x_test_dl)\n",
    "\n",
    "    DL2prob = DL2model.predict(x_test_dl)\n",
    "    DL2prob = DL2prob[:,1:]\n",
    "    DL2pred = np.argmax(DL2prob, axis=1)\n",
    "    DL2pred = DL2pred + 1\n",
    "\n",
    "    \n",
    "    MLpred = ML1model.predict(testing_data_rf)\n",
    "    MLprob = ML1model.predict_proba(testing_data_rf)\n",
    "    \n",
    "    ML2pred = ML2model.predict(testing_data_rf)\n",
    "    ML2prob = ML2model.predict_proba(testing_data_rf)\n",
    "    \n",
    "    scores += [accuracy_score(MLpred,dl_prediction_labels)]\n",
    "    scores += [accuracy_score(ML2pred,dl_prediction_labels)]\n",
    "    scores += [accuracy_score(DL1pred,dl_prediction_labels)]\n",
    "    scores += [accuracy_score(DL2pred,dl_prediction_labels)]\n",
    "\n",
    "    print(\"Scores: \", scores)\n",
    "    \n",
    "    \n",
    "    #Ensemble learning/voting system\n",
    "    final_pred = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(MLpred)):\n",
    "#         print(\"DL2pred: \", DL2pred[i], \"DL1pred: \", DL1pred[i], \"MLpred: \", MLpred[i])\n",
    "        if MLpred[i]==2:\n",
    "            MLpred[i] = 0\n",
    "            \n",
    "        if ML2pred[i]==2:\n",
    "            ML2pred[i] = 0\n",
    "            \n",
    "        final_pred.append(MLpred[i])\n",
    "        final_pred.append(ML2pred[i])\n",
    "        final_pred.append(DL1pred[i])\n",
    "        final_pred.append(DL2pred[i])\n",
    "\n",
    "        print(\"final_pred:\", final_pred)\n",
    "        to_pass = final_pred\n",
    "        \n",
    "        most_freq_item, occ_cnt = most_frequent(final_pred)\n",
    "\n",
    "        if occ_cnt[0]==occ_cnt[1]:\n",
    "            # soft voting\n",
    "            DL1prob_max = DL1prob.max()\n",
    "            MLprob_max = MLprob.max()\n",
    "            ML2prob_max = ML2prob.max()\n",
    "        \n",
    "            prob_all = []\n",
    "            prob_all.append(MLprob_max)\n",
    "            prob_all.append(ML2prob_max)\n",
    "            prob_all.append(DL1prob_max)\n",
    "            prob_all.append(DL2prob[i])\n",
    "            \n",
    "            prob_all_max = np.argmax(prob_all)\n",
    "#             print(\"prob_all : \",prob_all)\n",
    "#             print(\"Prob_all_max:\", prob_all_max)\n",
    "            return_ele =final_pred[prob_all_max]\n",
    "            \n",
    "#             print(\"return_ele: \",return_ele)\n",
    "            final_pred =[]\n",
    "            final_pred.append(return_ele)\n",
    "            \n",
    "        else:\n",
    "            # hard voting\n",
    "            final_pred = []\n",
    "            final_pred.append(most_freq_item)\n",
    "    \n",
    "    #Outputs\n",
    "    final_pred = np.array(final_pred)\n",
    "    dl_prediction_labels = np.array(dl_prediction_labels)\n",
    "    \n",
    "#     score = accuracy_score(final_pred,dl_prediction_labels)\n",
    "#     cm = confusion_matrix(final_pred,dl_prediction_labels)\n",
    "#     print(\"confusion_matrix\")\n",
    "#     print(cm)\n",
    "    \n",
    "    \n",
    "#     fpr_log, tpr_log, thresholds_log = roc_curve(dl_prediction_labels, MLpred)\n",
    "#     fpr_knn, tpr_knn, thresholds_knn = roc_curve(dl_prediction_labels, ML2pred)\n",
    "#     fpr_tree, tpr_tree, thresholds_tree = roc_curve(dl_prediction_labels, DL1pred)\n",
    "#     fpr_rfc, tpr_rfc, thresholds_rfc = roc_curve(dl_prediction_labels, DL1pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     fpr_ens, tpr_ens, thresholds_ens = roc_curve(dl_prediction_labels, final_pred)\n",
    "#     plt.plot(fpr_log, tpr_log,label = 'ML1')\n",
    "#     plt.plot(fpr_knn, tpr_knn,label = 'ML2')\n",
    "#     plt.plot(fpr_tree, tpr_tree,label = 'DL1')\n",
    "#     plt.plot(fpr_rfc, tpr_rfc,label = 'DL2')\n",
    "    \n",
    "#     plt.plot(fpr_ens, tpr_ens,label = 'Ensemble')\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.legend()\n",
    "#     plt.title('ROC Curve no GridSearch')\n",
    "#     plt.show()\n",
    "#     print(\"AUC Ensemble: {}\".format(roc_auc_score(dl_prediction_labels, final_pred)))\n",
    "    return final_pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.0, 0.0, 0.0, 1.0]\n",
      "final_pred: [0, 0, 0, 1]\n",
      "occurence_count:  Counter({0: 3, 1: 1})\n",
      "final output:  [0]\n",
      "final score:  0.0\n"
     ]
    }
   ],
   "source": [
    "dl_prediction_labels = pred_class\n",
    "\n",
    "final_pred, score = test_el(x_test_dl, testing_data_rf, dl_prediction_labels)\n",
    "\n",
    "print(\"final output: \", final_pred)\n",
    "print(\"final score: \", score)\n",
    "# print(\"confusion_matrix:\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
